{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score,precision_score,classification_report,roc_auc_score,roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the data into training and testing data\n",
    "\n",
    "X = df\n",
    "y = target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization using sklearn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_norm = norm.transform(X_train)\n",
    "\n",
    "# transform testing dataabs\n",
    "X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standarization with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "\n",
    "#numerical features\n",
    "num_cols = ['Item_Weight','Item_Visibility','Item_MRP','Outlet_Establishment_Year']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # fit on training data column\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the testing data column\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or this method\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "#X_train_scaled = scaler.fit_transform(X_train)\n",
    "#X_train = pd.DataFrame(X_train_scaled)\n",
    "\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "#X_test = pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score,precision_score,classification_report,roc_auc_score,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEARCH FOR OPTIMUN PARAMETERS USING GRIDSEARCH\n",
    "params = {'penalty':['l1','l2'],'C':[0.01,0.1,1,10,100],'class_weight':['balanced',None]}\n",
    "logistic_clf = GridSearchCV(LogisticRegression(),param_grid=params,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the classifier\n",
    "logistic_clf.fit(X_train,y_train)\n",
    "\n",
    "logistic_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "logistic_predict = logistic_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_accuracy = accuracy_score(y_test,logistic_predict)\n",
    "print(f\"Using logistic regression we get an accuracy of {round(log_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,logistic_predict)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,logistic_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_f1 = f1_score(y_test, logistic_predict)\n",
    "print(f'The f1 score for logistic regression is {round(logistic_f1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and AUC \n",
    "probs = logistic_clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "log_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "# plot curve\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.title(f\"AUC = {round(log_auc,3)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimun parameters using gridsearch\n",
    "params= {'n_neighbors': np.arange(1, 10)}\n",
    "grid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = params, \n",
    "                           scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "knn_clf = GridSearchCV(KNeighborsClassifier(),params,cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "knn_clf.fit(X_train,y_train)\n",
    "knn_clf.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "knn_predict = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "knn_accuracy = accuracy_score(y_test,knn_predict)\n",
    "print(f\"Using k-nearest neighbours we get an accuracy of {round(knn_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,knn_predict)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,knn_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_f1 = f1_score(y_test, knn_predict)\n",
    "print(f'The f1 score for K nearest neignbours is {round(knn_f1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and AUC \n",
    "probs = knn_clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "knn_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "# plot curve\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.title(f\"AUC = {round(knn_auc,3)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree= DecisionTreeClassifier(random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for optimum parameters\n",
    "params = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n",
    "          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}\n",
    "tree_clf = GridSearchCV(dtree, param_grid=params, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tree_clf.fit(X_train,y_train)\n",
    "tree_clf.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "tree_predict = tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "tree_accuracy = accuracy_score(y_test,tree_predict)\n",
    "print(f\"Using Decision Trees we get an accuracy of {round(tree_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,tree_predict)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,tree_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_f1 = f1_score(y_test, tree_predict)\n",
    "print(f'The f1 score Descision trees is {round(tree_f1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and AUC \n",
    "probs = tree_clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "tree_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "# plot curve\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.title(f\"AUC = {round(tree_auc,3)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for optimum parameters\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm_clf = GridSearchCV(SVC(kernel='rbf', probability=True), param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "svm_clf.fit(X_train,y_train)\n",
    "svm_clf.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "svm_predict = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "svm_accuracy = accuracy_score(y_test,svm_predict)\n",
    "print(f\"Using SVM we get an accuracy of {round(svm_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,svm_predict)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,svm_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_f1 = f1_score(y_test, svm_predict)\n",
    "print(f'The f1 score for SVM is {round(svm_f1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and AUC \n",
    "probs = svm_clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "svm_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "# plot curve\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.title(f\"AUC = {round(svm_auc,3)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    \"Logistic regression\":{'Accuracy':log_accuracy, 'AUC':log_auc, 'F1 score':logistic_f1},\n",
    "    \"K-nearest neighbours\":{'Accuracy':knn_accuracy, 'AUC':knn_auc, 'F1 score':knn_f1},\n",
    "    \"Decision trees\":{'Accuracy':tree_accuracy, 'AUC':tree_auc, 'F1 score':tree_f1},\n",
    "    \"Support vector machine\":{'Accuracy':svm_accuracy, 'AUC':svm_auc, 'F1 score':svm_f1}\n",
    "}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15)\n",
    "titles = ['AUC','Accuracy','F1 score']\n",
    "for title,label in enumerate(comparison.columns):\n",
    "    plt.subplot(2,2,title+1)\n",
    "    sns.barplot(x=comparison.index, y = comparison[label], data=comparison)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.title(titles[title])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
